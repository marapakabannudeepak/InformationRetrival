{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4lBh8iv0uz6",
    "outputId": "47dd3bb6-82d0-45eb-b885-ad66d578a371"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    " #nltk.download('stopwords')\n",
    " #nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqq0mXKqNZNE"
   },
   "outputs": [],
   "source": [
    "def snow_ball_stemmer(data_list):\n",
    "  stem = SnowballStemmer(language='english')\n",
    "  return list(map(lambda word:stem.stem(word),data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1Shf-LGL9oA"
   },
   "outputs": [],
   "source": [
    "def text_cleaner(data_str):\n",
    "  data_str = data_str.strip(' ').lower()\n",
    "  data_str = data_str\n",
    "  new_str= ''\n",
    "  for char in data_str:\n",
    "    if char not in '''!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~''':\n",
    "      new_str +=char\n",
    "  return new_str.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOT4W6_GHG_z"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Takes a list of words as parameter and returns a new list after removing the stopwords\n",
    "def remove_stop_words(data_list):\n",
    "  new_list = []\n",
    "  for word in data_list:\n",
    "    if word not in nltk.corpus.stopwords.words('english'):\n",
    "      new_list.append(word)\n",
    "  return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVvXyHvuAfLx"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "  return remove_stop_words(nltk.word_tokenize(text_cleaner(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txQ9VkmoS_EQ"
   },
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "# cat 3\n",
    "# 'cat' = [1,2,5]\n",
    "\n",
    "# Takes a term and its corresponding documentID and adds to the inverted index\n",
    "def add_term_to_inverted_index(term,documentID):\n",
    "  # an inverted index is a dict contained term,globalCount to posting list and each posting list contains ()\n",
    "  try:\n",
    "    # print(inverted_index[term]['count'])\n",
    "    # inverted_index[term]['count']+=1 # increment the term count\n",
    "    \n",
    "    for document in inverted_index[term]['posting_list'].copy():\n",
    "      if document['docID']>documentID:\n",
    "      # if documentID in [i['docID'] for i in inverted_index[term]['posting_list']]:\n",
    "        # we don't have that particular document in the posting list, add it\n",
    "        inverted_index[term]['posting_list'].append({'docID':documentID,'count':1})\n",
    "        inverted_index[term]['count']=len(inverted_index[term]['posting_list'])\n",
    "        inverted_index[term]['posting_list'] = sorted(inverted_index[term]['posting_list'],key=lambda x:x['docID'])\n",
    "        break\n",
    "      elif document['docID']==documentID:\n",
    "        document['count']+=1\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "      inverted_index[term]['posting_list'].append({'docID':documentID,'count':1})\n",
    "      inverted_index[term]['count']=len(inverted_index[term]['posting_list'])\n",
    "      inverted_index[term]['posting_list'] = sorted(inverted_index[term]['posting_list'],key=lambda x:x['docID'])\n",
    "\n",
    "\n",
    "\n",
    "  except KeyError:\n",
    "    # print('we failed')\n",
    "    # We are adding a term to our index keep the count empty and an empty list for the posting_list\n",
    "    inverted_index[term]={'count':1,'posting_list':[{'docID':documentID,'count':1}]}\n",
    "    # print(inverted_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvdKTO6C8DAx"
   },
   "outputs": [],
   "source": [
    "# for temp1 in inverted_index.copy().keys():\n",
    "#   inverted_index[temp1]['count']=len(inverted_index[temp1]['posting_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VCCEnkxgxLx"
   },
   "outputs": [],
   "source": [
    "# Convert all unknown attributes to NaN\n",
    "dataframe_original = pd.read_csv(r\"https://drive.google.com/file/d/1SigikUJF9OCigB7juNrx8R8EkKX_Cx0l/view?usp=sharing\")\n",
    "\n",
    "dataframe = dataframe_original.replace(['Unknown','unknown'],'')\n",
    "# dataframe.drop('Wiki Page',1,inplace=True)\n",
    "dataframe.drop(columns=\"Wiki Page\",inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4z4Doikai1La"
   },
   "outputs": [],
   "source": [
    "# debug head(10)\n",
    "total_doc=5000\n",
    "retried_doc=30\n",
    "\n",
    "for temp in dataframe.head(total_doc).iterrows():\n",
    "  value = ''\n",
    "\n",
    "  docID = temp[0]\n",
    " \n",
    "  for temp_value in temp[1].to_dict().values():\n",
    "    value+=str(temp_value)+' ' \n",
    "\n",
    "  value = value.replace('nan','')\n",
    "  processed_list = preprocess(value)\n",
    "\n",
    "  # change to a lemmatizer instead\n",
    "  # print(snow_ball_stemmer(processed_list))\n",
    "  # print(processed_list)\n",
    "  for term in processed_list:\n",
    "    add_term_to_inverted_index(term,docID)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfkLg5guLKWi",
    "outputId": "d7341fb0-dd5c-47aa-eb81-20be91608d87"
   },
   "outputs": [],
   "source": [
    "# inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nh1PKlKZUGcy"
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOKspb_YYbwy"
   },
   "outputs": [],
   "source": [
    "def idf_func(c):\n",
    "  \n",
    "  return math.log2(total_doc/c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xerNSPjlrNZf"
   },
   "outputs": [],
   "source": [
    "def calc_idf():\n",
    "  for term in inverted_index.keys():\n",
    "    c=inverted_index[term]['count']\n",
    "    idf_value=idf_func(c)\n",
    "    inverted_index[term]['idf']=idf_value\n",
    "calc_idf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_Cq7rFs4a0J"
   },
   "source": [
    "#CALCULATING TF-IDF SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiJAec-Uxfrk"
   },
   "outputs": [],
   "source": [
    "tfidf_dict={}\n",
    "tfidf_list=[]\n",
    "\n",
    "for term in inverted_index.keys():\n",
    "  tfidf_list.append(term)\n",
    "  temp=inverted_index[term]['posting_list']\n",
    "  for t in temp:\n",
    "    tfidf_dict[t['docID'],term]=inverted_index[term]['idf']*(1+math.log2(t['count']))\n",
    "    \n",
    "\n",
    "# print(tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1ls_mO0cG1J"
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWOtcMAM1Bhs"
   },
   "outputs": [],
   "source": [
    "D = np.zeros((total_doc,len(inverted_index)))\n",
    "for i in tfidf_dict:\n",
    "  k=tfidf_list.index(i[1])\n",
    "  D[i[0]][k] = tfidf_dict[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PDfQ4qj4Slp"
   },
   "source": [
    "#Calculating Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKrgqODVYV2k"
   },
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity1(query_dict):\n",
    "\n",
    " \n",
    "\n",
    "  Q=  np.zeros((len(inverted_index)))\n",
    "  res=[]\n",
    "  \n",
    "  for i in inverted_index.keys():\n",
    "     k=tfidf_list.index(i)    \n",
    "     if i in query_dict.keys():\n",
    "       Q[k]=query_dict[i]\n",
    "\n",
    "  \n",
    "  for row in range(D.shape[0]):\n",
    "    # print(row,\"size:\",row.shape)\n",
    "\n",
    "    # print(Q,\"Query Size:\",Q.shape)\n",
    "\n",
    "    # break\n",
    "    #res.append(cosine_similarity(D[0],Q))\n",
    "    result= 1 - spatial.distance.cosine(D[row], Q)\n",
    "    res.append([row,result])\n",
    "  \n",
    "  return res\n",
    " \n",
    "  # print(D)\n",
    "  # print(Q)\n",
    "  # print(\"Shape of D\",D.shape)\n",
    "  # print(\"shape of Q\",Q.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBmi9i7E4NKB"
   },
   "source": [
    "#ENTER THE QUERY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHG7K-XzXrnv",
    "outputId": "3ae4eeca-0497-4f02-8a1a-2ad0971507ed"
   },
   "outputs": [],
   "source": [
    "query=input(\"Enter your query:-\")\n",
    "q_list= preprocess(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ipm9OzFYPaj"
   },
   "outputs": [],
   "source": [
    "query_dict={}\n",
    "\n",
    "# q_list=query.split()\n",
    "#print(q_list)\n",
    "for q in q_list:\n",
    "  \n",
    "  if q  in inverted_index.keys():\n",
    "     query_dict[q]=(inverted_index[q]['idf'])*(1+math.log2(q_list.count(q)))\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "   \n",
    "res=cosine_similarity1(query_dict)\n",
    "sorted_res= sorted(res,key=lambda x:x[1],reverse=True)\n",
    "# print(res,'\\n')\n",
    "# print(sorted_res,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0L5Se6y5l92"
   },
   "source": [
    "#Displaying the retrived documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ReWYcCTsoNx5",
    "outputId": "2cd61689-968b-4377-ce13-7485c8b6fbd9"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "dataframe.iloc[[docid[0] for docid in sorted_res]].head(retried_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPvN9tiK0Vl8"
   },
   "source": [
    "# Relavence feedback\n",
    "## Recall and Precision\n",
    "## Recall = #(relevant retrieved)/#(relevant) -> tp/tp+fn\n",
    "\n",
    "## Precision = #(relevant retrieved)/#(retrieved) -> tp/tp+fp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5ZlIGlCOqhI",
    "outputId": "00d0ad2e-b86d-40a0-e870-dce543bb32ff"
   },
   "source": [
    "# print('Enter 1 if the document is relevant\\n      0 if the document is irrelevant')\n",
    "feedback=[]\n",
    "for f in range(retried_doc):\n",
    "  print('Is the document', f ,'relevant')\n",
    "  x=int(input())\n",
    "  feedback.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG3Mgywt0kPl"
   },
   "source": [
    "\n",
    "### Enter 1 if the document is relevant\n",
    "### Enter 0 if the document is irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Enter 1 if the document is relevant\\n      0 if the document is irrelevant')\n",
    "feedback=[]\n",
    "for f in range(retried_doc):\n",
    "  print('Is the document', f ,'relevant')\n",
    "  x=int(input(0))\n",
    "  feedback.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IiXhonK9amsw"
   },
   "outputs": [],
   "source": [
    "def evaluate(feedback):\n",
    "  retrieved=30\n",
    "  recall=[]\n",
    "  precision=[]\n",
    "  relevant=feedback.count(1) \n",
    "  relavent_count=0\n",
    "  for ct in range(retried_doc):\n",
    "  \n",
    "    if feedback[ct]==1:\n",
    "      relavent_count+=1;\n",
    "    recall.append(relavent_count/relevant)\n",
    "    precision.append(relavent_count/(ct+1))\n",
    "  print(\"RECALL\",recall)\n",
    "  print(\"PRECISION\",precision)\n",
    "  plt.xlabel(\"RECALL\")\n",
    "  plt.ylabel(\"PRECISION\")\n",
    "  plt.title(\"P-R Curve\")\n",
    "  plt.plot(recall,precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "01LuInSb0Qr0",
    "outputId": "1e4f8dbf-6975-44a5-b3f2-7153f5274088"
   },
   "outputs": [],
   "source": [
    "evaluate(feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNwQN89ZzmRm"
   },
   "source": [
    "#Non Trivial Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GB00zOxlzxoM"
   },
   "source": [
    "Filtering movies based on release year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2K-gXxI1AM3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# enter filters in the query string using /. Example:- classical singer /year 1950 \n",
    "def query_with_filters(q_string):\n",
    "  arr = q_string.split(' ')\n",
    "  try:\n",
    "    year_index = arr.index('/year')\n",
    "    if arr[year_index+1].isdigit():\n",
    "      input_dataframe = dataframe[dataframe['Release Year']==int(arr[year_index+1])]\n",
    "      q_string = \" \".join(arr[:year_index])\n",
    "      arr =arr[:year_index]\n",
    "    # return query,input_dataframe\n",
    "    \n",
    "  except ValueError:\n",
    "    pass\n",
    "\n",
    "\n",
    "  try:\n",
    "    arr = q_string.split(' ')\n",
    "    year_index = arr.index('/genre')\n",
    "    if arr[year_index+1]:\n",
    "      input_dataframe = dataframe[dataframe['Genre']==str(arr[year_index+1]).lower()]\n",
    "      query = \" \".join(arr[:year_index])\n",
    "      return query,input_dataframe\n",
    "  except ValueError:\n",
    "    pass\n",
    "\n",
    "\n",
    "  return q_string,input_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MldWiTTY6g9O",
    "outputId": "f07bbbd8-b7d1-47fa-ac0b-f6e6409d10d1"
   },
   "outputs": [],
   "source": [
    "print(query_with_filters('pop singer /year 1970 /genre comedy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IR PROJECT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f0eeec13cea5cfede3f550a810d22afafc30bcf37ef38b1346a8398fb774e013"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
